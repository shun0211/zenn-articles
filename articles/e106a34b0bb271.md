---
title: "Corrective RAGï¼ˆCRAGï¼‰ã®æ¦‚å¿µã¨å®Ÿè£…æ–¹æ³•"
emoji: "ğŸ”"
type: "tech"
topics: ["RAG", "LangChain", "LangGraph", "ç”ŸæˆAI", "AI"]
published: true
---

ã“ã‚“ã«ã¡ã¯ã€é…’äº•ã§ã™ï¼
[æ ªå¼ä¼šç¤¾ EGGHEAD]([https://egghead.co.jp](https://egghead.co.jp?utm_source=zenn&utm_medium=social&utm_campaign=normal))ï¼ˆã‚¨ãƒƒã‚°ãƒ˜ãƒƒãƒ‰ï¼‰ã¨ã„ã†ã€Œè£½é€ æ¥­ã§ç”Ÿæˆ AI ã‚’æ´»ç”¨ã—ãŸã‚·ã‚¹ãƒ†ãƒ é–‹ç™ºã€ã‚’ã—ã¦ã„ã‚‹ä¼šç¤¾ã®ä»£è¡¨ã‚’ã—ã¦ãŠã‚Šã¾ã™ã€‚

https://egghead.co.jp?utm_source=zenn&utm_medium=social&utm_campaign=normal

## ã¯ã˜ã‚ã«

æœ¬è¨˜äº‹ã§ã¯ã€æ¤œç´¢çµæœã‚’è‡ªå·±è©•ä¾¡ã—ã€å¿…è¦ã«å¿œã˜ã¦è¿½åŠ æƒ…å ±æºã‚’æ´»ç”¨ã™ã‚‹ã€ŒCorrective RAGï¼ˆCRAGï¼‰ã€ã«ã¤ã„ã¦è§£èª¬ã—ã€LangGraph ã‚’ä½¿ã£ãŸå®Ÿè£…æ–¹æ³•ã‚’ç´¹ä»‹ã—ã¾ã™ã€‚

https://arxiv.org/abs/2401.15884

## Corrective RAGï¼ˆCRAGï¼‰ã¨ã¯

### CRAG ã®åŸºæœ¬æ¦‚å¿µ

Corrective RAGï¼ˆCRAGï¼‰ã¯ã€å¾“æ¥ã® RAG ã‚·ã‚¹ãƒ†ãƒ ã‚’æ‹¡å¼µã—ã€æ¤œç´¢çµæœãŒãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã¨é–¢é€£ã—ã¦ã„ã‚‹ã‹ã‚’è©•ä¾¡(è‡ªå·±åçœ)ã™ã‚‹ã“ã¨ã§RAGã®ç²¾åº¦ã‚’ä¸Šã’ã‚‹ãŸã‚ã®æˆ¦ç•¥ã§ã™ã€‚CRAG ã§ã¯æ¤œç´¢ã•ã‚ŒãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ã€ŒCorrect(æ­£ã—ã„)ã€ã€ã€ŒIncorrectï¼ˆä¸æ­£ç¢ºï¼‰ã€ã€ã€ŒAmbiguousï¼ˆæ›–æ˜§ï¼‰ã€ã¨ä¸‰ã¤ã«è©•ä¾¡ã—ã¾ã™ã€‚

#### è©•ä¾¡å¾Œã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
**Correct**
æ¤œç´¢çµæœãŒæ­£ã—ã„ã¨åˆ¤æ–­ã•ã‚ŒãŸå ´åˆã€æ¤œç´¢ã•ã‚ŒãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’çŸ¥è­˜åˆ†è§£ã€ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã€å†æ§‹æˆã«ã‚ˆã£ã¦ã€ã‚ˆã‚Šæ­£ç¢ºãªçŸ¥è­˜ã«çµã‚Šè¾¼ã¿ã¾ã™ã€‚

**Incorrect**
æ¤œç´¢çµæœãŒä¸æ­£ç¢ºã ã¨åˆ¤æ–­ã•ã‚ŒãŸå ´åˆã€æ¤œç´¢ã•ã‚ŒãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ç ´æ£„ã—ã€ã‚¦ã‚§ãƒ–æ¤œç´¢ã‚’çŸ¥è­˜æºã¨ã—ã¦åˆ©ç”¨ã—ã¾ã™ã€‚

**Ambiguous**
åˆ¤æ–­ãŒé›£ã—ã„å ´åˆã€Correctã¨Incorrectã®ä¸¡æ–¹ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’çµ„ã¿åˆã‚ã›ã¦ã€å†…éƒ¨çŸ¥è­˜ã¨å¤–éƒ¨çŸ¥è­˜ã‚’çµ„ã¿åˆã‚ã›ã¾ã™ã€‚

ãã—ã¦ã€æ¤œç´¢ã•ã‚ŒãŸçŸ¥è­˜ã‚’å†æ§‹ç¯‰(decompose-then-recompose)ã—ã¦ã€ã•ã‚‰ã«ç²¾åº¦ã‚’é«˜ã‚ã¾ã™ã€‚ã“ã‚Œã‚’ã—ãŸå¾Œã«ãã®çŸ¥è­˜ã¨ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã‹ã‚‰ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆã—ã¦LLMã‚’ä½¿ã£ã¦å›ç­”ã‚’ç”Ÿæˆã—ã¾ã™ã€‚

### CRAG ã®ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼

CRAG ã®åŸºæœ¬çš„ãªãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã¯ä»¥ä¸‹ã®ã‚¹ãƒ†ãƒƒãƒ—ã§æ§‹æˆã•ã‚Œã¾ã™ã€‚

1. ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®è³ªå•ã‚’å—ã‘å–ã‚‹
2. ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰é–¢é€£æ–‡æ›¸ã‚’æ¤œç´¢ã™ã‚‹
3. æ¤œç´¢çµæœã®é–¢é€£æ€§ã‚’è©•ä¾¡ã™ã‚‹
4. é–¢é€£æ€§ã®é«˜ã„æ–‡æ›¸ãŒå­˜åœ¨ã™ã‚‹å ´åˆã¯å›ç­”ã‚’ç”Ÿæˆã™ã‚‹
5. é–¢é€£æ€§ã®é«˜ã„æ–‡æ›¸ãŒä¸è¶³ã—ã¦ã„ã‚‹å ´åˆã¯ã€ã‚¯ã‚¨ãƒªã‚’æœ€é©åŒ–ã—ã¦ Web æ¤œç´¢ã‚’è¡Œã†
6. Web æ¤œç´¢çµæœã‚’å«ã‚ã¦å›ç­”ã‚’ç”Ÿæˆã™ã‚‹

![](/images/c-rag/image-1.png)

â€» ç°¡ç•¥åŒ–ã®ãŸã‚ã€ã‚ã„ã¾ã„(Ambiguous)ã®å ´åˆã‚’çœç•¥ã—ã¦ã„ã¾ã™ã€‚å®Ÿéš›ã®è«–æ–‡ä¸­ã®å›³ãŒä»¥ä¸‹ã§ã™ã€‚

![](/images/c-rag/image-3.png)

### å¾“æ¥ã® RAG ã¨ã®é•ã„

å¾“æ¥ã® RAG ã‚·ã‚¹ãƒ†ãƒ ã¨æ¯”è¼ƒã—ãŸ CRAG ã®ä¸»ãªé•ã„ã¯ä»¥ä¸‹ã®é€šã‚Šã§ã™ã€‚

| ç‰¹å¾´ | å¾“æ¥ã®RAG | CRAG |
|------|----------|------|
| **æ¤œç´¢çµæœã®ä½¿ã„æ–¹** | æ¤œç´¢çµæœã‚’ãã®ã¾ã¾ä½¿ã† | æ¤œç´¢çµæœãŒè³ªå•ã«é–¢ä¿‚ã‚ã‚‹ã‹ç¢ºèªã™ã‚‹ |
| **æƒ…å ±ã®æ•´ç†** | ç‰¹ã«è¡Œã‚ãªã„ | æ–‡æ›¸ã‚’å°ã•ãåˆ†ã‘ã€å¿…è¦ãªæƒ…å ±ã ã‘ã‚’é¸ã³å‡ºã™ |
| **å¤–éƒ¨æƒ…å ±ã®åˆ©ç”¨** | ç‰¹ã«è¡Œã‚ãªã„ | æ¤œç´¢çµæœã ã‘ã§ã¯è¶³ã‚Šãªã„å ´åˆã€Webãªã©ã‹ã‚‰è¿½åŠ æƒ…å ±ã‚’é›†ã‚ã‚‹ |
| **æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®æ”¹å–„** | ç‰¹ã«è¡Œã‚ãªã„ | ã‚ˆã‚Šè‰¯ã„æ¤œç´¢çµæœã‚’å¾—ã‚‹ãŸã‚ã«æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å·¥å¤«ã™ã‚‹ |

## CRAG ã®å®Ÿè£…

### å®Ÿè£…æº–å‚™

Google Colab ã§ã®å®Ÿè£…ã‚’å‰æã¨ã—ã¾ã™ã€‚ãªãŠã€ã“ã“ã‹ã‚‰ã¯LangGraphã®ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã‚’è¸è¥²ã—ã¦ã„ã¾ã™ã€‚

https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_crag/

#### å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

CRAG ã‚’å®Ÿè£…ã™ã‚‹ãŸã‚ã«ã€ä»¥ä¸‹ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¾ã™ã€‚

```bash
pip install langchain_community tiktoken langchain-openai langchainhub chromadb langchain langgraph tavily-python
```

#### API ã‚­ãƒ¼ã®è¨­å®š

OpenAI API ã¨ Tavily Search API ã®ã‚­ãƒ¼ã‚’è¨­å®šã—ã¾ã™ã€‚

```python
import getpass
import os

def _set_env(key: str):
    if key not in os.environ:
        os.environ[key] = getpass.getpass(f"{key}:")

_set_env("OPENAI_API_KEY")
_set_env("TAVILY_API_KEY")
```

#### ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ä½œæˆ

ãƒ†ã‚¹ãƒˆç”¨ã®ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã€åƒ•ã®[å‰ã®ãƒ–ãƒ­ã‚°è¨˜äº‹](https://zenn.dev/shun_sakai/articles/22966db52b604d)ã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åŒ–ã—ã¾ã™ã€‚

```python
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import WebBaseLoader
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings

urls = [
    "https://zenn.dev/shun_sakai/articles/22966db52b604d",
]

docs = [WebBaseLoader(url).load() for url in urls]
docs_list = [item for sublist in docs for item in sublist]

text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
    chunk_size=250, chunk_overlap=0
)
doc_splits = text_splitter.split_documents(docs_list)

# ãƒ™ã‚¯ãƒˆãƒ«DBã«è¿½åŠ 
vectorstore = Chroma.from_documents(
    documents=doc_splits,
    collection_name="rag-chroma",
    embedding=OpenAIEmbeddings(),
)
retriever = vectorstore.as_retriever()
```

### CRAG ã®ä¸»è¦ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ

#### æ¤œç´¢çµæœã®è©•ä¾¡æ©Ÿèƒ½

æ¤œç´¢çµæœã®é–¢é€£æ€§ã‚’è©•ä¾¡ã™ã‚‹ãŸã‚ã®ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’å®Ÿè£…ã—ã¾ã™ã€‚

```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from pydantic import BaseModel, Field

# ãƒ‡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒ«
class GradeDocuments(BaseModel):
    """æ¤œç´¢ã•ã‚ŒãŸæ–‡æ›¸ã®é–¢é€£æ€§ã‚’è©•ä¾¡ã™ã‚‹ãŸã‚ã®ãƒã‚¤ãƒŠãƒªã‚¹ã‚³ã‚¢"""
    binary_score: str = Field(
        description="æ–‡æ›¸ãŒè³ªå•ã«é–¢é€£ã—ã¦ã„ã‚‹ã‹ã©ã†ã‹ã€'yes'ã¾ãŸã¯'no'ã§è©•ä¾¡"
    )

# é–¢æ•°å‘¼ã³å‡ºã—æ©Ÿèƒ½ã‚’æŒã¤LLM
llm = ChatOpenAI(model="gpt-3.5-turbo-0125", temperature=0)
structured_llm_grader = llm.with_structured_output(GradeDocuments)

# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
system = """ã‚ãªãŸã¯æ¤œç´¢ã•ã‚ŒãŸæ–‡æ›¸ãŒãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«é–¢é€£ã—ã¦ã„ã‚‹ã‹ã‚’è©•ä¾¡ã™ã‚‹æ¡ç‚¹è€…ã§ã™ã€‚
    æ–‡æ›¸ã«è³ªå•ã«é–¢é€£ã™ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚„æ„å‘³çš„ãªå†…å®¹ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã€é–¢é€£æ€§ãŒã‚ã‚‹ã¨è©•ä¾¡ã—ã¦ãã ã•ã„ã€‚
    æ–‡æ›¸ãŒè³ªå•ã«é–¢é€£ã—ã¦ã„ã‚‹ã‹ã©ã†ã‹ã‚’'yes'ã¾ãŸã¯'no'ã®ãƒã‚¤ãƒŠãƒªã‚¹ã‚³ã‚¢ã§ç¤ºã—ã¦ãã ã•ã„ã€‚"""
grade_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system),
        ("human", "æ¤œç´¢ã•ã‚ŒãŸæ–‡æ›¸: \n\n {document} \n\n ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•: {question}"),
    ]
)

retrieval_grader = grade_prompt | structured_llm_grader
```

ã“ã®ã‚³ãƒ¼ãƒ‰ã§ã¯ã€Pydantic ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦æ§‹é€ åŒ–ã•ã‚ŒãŸå‡ºåŠ›ã‚’å®šç¾©ã—ã€LLM ã«æ–‡æ›¸ã®é–¢é€£æ€§ã‚’è©•ä¾¡ã•ã›ã¦ã„ã¾ã™ã€‚

#### ã‚¯ã‚¨ãƒªæ›¸ãæ›ãˆæ©Ÿèƒ½

æ¤œç´¢çµæœãŒä¸ååˆ†ãªå ´åˆã«ã€ã‚ˆã‚ŠåŠ¹æœçš„ãª Web æ¤œç´¢ã®ãŸã‚ã«ã‚¯ã‚¨ãƒªã‚’æœ€é©åŒ–ã—ã¾ã™ã€‚

```python
# LLM
llm = ChatOpenAI(model="gpt-3.5-turbo-0125", temperature=0)

# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
system = """ã‚ãªãŸã¯è³ªå•ã‚’æ›¸ãæ›ãˆã¦ã€Webæ¤œç´¢ã«æœ€é©åŒ–ã•ã‚ŒãŸå½¢å¼ã«å¤‰æ›ã™ã‚‹å°‚é–€å®¶ã§ã™ã€‚
     å…¥åŠ›ã•ã‚ŒãŸè³ªå•ã‚’åˆ†æã—ã€ãã®èƒŒå¾Œã«ã‚ã‚‹æ„å‘³çš„ãªæ„å›³ã‚’ç†è§£ã—ã¦ã€ã‚ˆã‚Šè‰¯ã„æ¤œç´¢çµæœãŒå¾—ã‚‰ã‚Œã‚‹ã‚ˆã†ã«æ›¸ãæ›ãˆã¦ãã ã•ã„ã€‚"""
re_write_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system),
        (
            "human",
            "å…ƒã®è³ªå•: \n\n {question} \n ã‚ˆã‚Šè‰¯ã„è³ªå•ã«æ›¸ãæ›ãˆã¦ãã ã•ã„ã€‚",
        ),
    ]
)

from langchain_core.output_parsers import StrOutputParser
question_rewriter = re_write_prompt | llm | StrOutputParser()
```

#### Web æ¤œç´¢çµ±åˆæ©Ÿèƒ½

Tavily Search API ã‚’ä½¿ç”¨ã—ã¦ Web æ¤œç´¢ã‚’å®Ÿè£…ã—ã¾ã™ï¼š

```python
from langchain_community.tools.tavily_search import TavilySearchResults

web_search_tool = TavilySearchResults(k=3)
```

#### å›ç­”ç”Ÿæˆæ©Ÿèƒ½

æ¤œç´¢çµæœã‚’åŸºã«å›ç­”ã‚’ç”Ÿæˆã™ã‚‹æ©Ÿèƒ½ã‚’å®Ÿè£…ã—ã¾ã™ï¼š

```python
from langchain import hub
from langchain_core.output_parsers import StrOutputParser

# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
prompt = hub.pull("rlm/rag-prompt")

# LLM
llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0)

# å¾Œå‡¦ç†
def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

# ãƒã‚§ãƒ¼ãƒ³
rag_chain = prompt | llm | StrOutputParser()
```

### LangGraph ã«ã‚ˆã‚‹ CRAG ã®å®Ÿè£…

#### ã‚°ãƒ©ãƒ•çŠ¶æ…‹ã®å®šç¾©

LangGraph ã‚’ä½¿ç”¨ã—ã¦ CRAG ã®ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’å®Ÿè£…ã™ã‚‹ãŸã‚ã«ã€ã¾ãšã‚°ãƒ©ãƒ•ã®çŠ¶æ…‹ã‚’å®šç¾©ã—ã¾ã™ã€‚

```python
from typing import List
from typing_extensions import TypedDict

class GraphState(TypedDict):
    """
    ã‚°ãƒ©ãƒ•ã®çŠ¶æ…‹ã‚’è¡¨ç¾ã—ã¾ã™ã€‚

    å±æ€§:
        question: è³ªå•
        generation: LLMã®ç”Ÿæˆçµæœ
        web_search: Webæ¤œç´¢ã‚’è¿½åŠ ã™ã‚‹ã‹ã©ã†ã‹
        documents: æ–‡æ›¸ã®ãƒªã‚¹ãƒˆ
    """

    question: str
    generation: str
    web_search: str
    documents: List[str]
```

#### ãƒãƒ¼ãƒ‰ã®å®Ÿè£…

ã‚°ãƒ©ãƒ•ã®ãƒãƒ¼ãƒ‰ã¨ãªã‚‹å„æ©Ÿèƒ½ã‚’å®Ÿè£…ã—ã¾ã™ã€‚

```python
from langchain.schema import Document

def retrieve(state):
    """
    æ–‡æ›¸ã‚’æ¤œç´¢ã—ã¾ã™

    Args:
        state (dict): ç¾åœ¨ã®ã‚°ãƒ©ãƒ•çŠ¶æ…‹

    Returns:
        state (dict): æ¤œç´¢ã•ã‚ŒãŸæ–‡æ›¸ã‚’å«ã‚€æ›´æ–°ã•ã‚ŒãŸçŠ¶æ…‹
    """
    print("---æ¤œç´¢ä¸­---")
    question = state["question"]

    # æ¤œç´¢
    documents = retriever.invoke(question)
    return {"documents": documents, "question": question}


def generate(state):
    """
    å›ç­”ã‚’ç”Ÿæˆã—ã¾ã™

    Args:
        state (dict): ç¾åœ¨ã®ã‚°ãƒ©ãƒ•çŠ¶æ…‹

    Returns:
        state (dict): ç”Ÿæˆã•ã‚ŒãŸå›ç­”ã‚’å«ã‚€æ›´æ–°ã•ã‚ŒãŸçŠ¶æ…‹
    """
    print("---å›ç­”ç”Ÿæˆä¸­---")
    question = state["question"]
    documents = state["documents"]

    # RAGç”Ÿæˆ
    generation = rag_chain.invoke({"context": documents, "question": question})
    return {"documents": documents, "question": question, "generation": generation}
```

ã“ã¡ã‚‰ã¯ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®è©•ä¾¡ã‚’ã™ã‚‹ãƒ¡ã‚½ãƒƒãƒ‰ã§ã™ãŒã€æŠ½å‡ºã—ãŸè¤‡æ•°ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®å†…ä¸€ã¤ã§ã‚‚é–¢ä¿‚ãªã„ã‚‚ã®ãŒå«ã¾ã‚Œã¦ã„ãŸã‚‰Webæ¤œç´¢ã™ã‚‹ä»•æ§˜ã«ãªã£ã¦ã„ã¾ã™ã€‚

```python
def grade_documents(state):
    """
    æ¤œç´¢ã•ã‚ŒãŸæ–‡æ›¸ãŒè³ªå•ã«é–¢é€£ã—ã¦ã„ã‚‹ã‹ã‚’è©•ä¾¡ã—ã¾ã™ã€‚

    Args:
        state (dict): ç¾åœ¨ã®ã‚°ãƒ©ãƒ•çŠ¶æ…‹

    Returns:
        state (dict): ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã•ã‚ŒãŸé–¢é€£æ–‡æ›¸ã‚’å«ã‚€æ›´æ–°ã•ã‚ŒãŸçŠ¶æ…‹
    """

    print("---æ–‡æ›¸ã®é–¢é€£æ€§ã‚’è©•ä¾¡ä¸­---")
    question = state["question"]
    documents = state["documents"]

    # å„æ–‡æ›¸ã‚’è©•ä¾¡
    filtered_docs = []
    web_search = "No"
    for d in documents:
        score = retrieval_grader.invoke(
            {"question": question, "document": d.page_content}
        )
        grade = score.binary_score
        if grade == "yes":
            print("---è©•ä¾¡: æ–‡æ›¸ã¯é–¢é€£æ€§ã‚ã‚Š---")
            filtered_docs.append(d)
        else:
            print("---è©•ä¾¡: æ–‡æ›¸ã¯é–¢é€£æ€§ãªã—---")
            web_search = "Yes"
            continue
    return {"documents": filtered_docs, "question": question, "web_search": web_search}


def transform_query(state):
    """
    ã‚ˆã‚Šè‰¯ã„è³ªå•ã«å¤‰æ›ã—ã¾ã™ã€‚

    Args:
        state (dict): ç¾åœ¨ã®ã‚°ãƒ©ãƒ•çŠ¶æ…‹

    Returns:
        state (dict): æ›¸ãæ›ãˆã‚‰ã‚ŒãŸè³ªå•ã‚’å«ã‚€æ›´æ–°ã•ã‚ŒãŸçŠ¶æ…‹
    """

    print("---ã‚¯ã‚¨ãƒªå¤‰æ›ä¸­---")
    question = state["question"]
    documents = state["documents"]

    # è³ªå•ã‚’æ›¸ãæ›ãˆ
    better_question = question_rewriter.invoke({"question": question})
    return {"documents": documents, "question": better_question}


def web_search(state):
    """
    æ›¸ãæ›ãˆã‚‰ã‚ŒãŸè³ªå•ã«åŸºã¥ã„ã¦Webæ¤œç´¢ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚

    Args:
        state (dict): ç¾åœ¨ã®ã‚°ãƒ©ãƒ•çŠ¶æ…‹

    Returns:
        state (dict): Webæ¤œç´¢çµæœã‚’è¿½åŠ ã—ãŸæ›´æ–°ã•ã‚ŒãŸçŠ¶æ…‹
    """

    print("---Webæ¤œç´¢ä¸­---")
    question = state["question"]
    documents = state["documents"]

    # Webæ¤œç´¢
    docs = web_search_tool.invoke({"query": question})
    web_results = "\n".join([d["content"] for d in docs])
    web_results = Document(page_content=web_results)
    documents.append(web_results)

    return {"documents": documents, "question": question}
```

#### ã‚¨ãƒƒã‚¸ã¨æ¡ä»¶åˆ†å²ã®è¨­å®š

ãƒãƒ¼ãƒ‰é–“ã®é·ç§»ã‚’æ±ºå®šã™ã‚‹æ¡ä»¶åˆ†å²ã‚’å®Ÿè£…ã—ã¾ã™ã€‚

```python
def decide_to_generate(state):
    """
    å›ç­”ã‚’ç”Ÿæˆã™ã‚‹ã‹ã€è³ªå•ã‚’å†ç”Ÿæˆã™ã‚‹ã‹ã‚’æ±ºå®šã—ã¾ã™ã€‚

    Args:
        state (dict): ç¾åœ¨ã®ã‚°ãƒ©ãƒ•çŠ¶æ…‹

    Returns:
        str: æ¬¡ã«å‘¼ã³å‡ºã™ãƒãƒ¼ãƒ‰ã‚’ç¤ºã™ãƒã‚¤ãƒŠãƒªæ±ºå®š
    """

    print("---è©•ä¾¡ã•ã‚ŒãŸæ–‡æ›¸ã‚’åˆ†æä¸­---")
    state["question"]
    web_search = state["web_search"]
    state["documents"]

    if web_search == "Yes":
        # ã™ã¹ã¦ã®æ–‡æ›¸ãŒãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã•ã‚ŒãŸå ´åˆ
        # æ–°ã—ã„ã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆã—ã¾ã™
        print(
            "---æ±ºå®š: ã™ã¹ã¦ã®æ–‡æ›¸ãŒè³ªå•ã«é–¢é€£ã—ã¦ã„ãªã„ãŸã‚ã€ã‚¯ã‚¨ãƒªã‚’å¤‰æ›ã—ã¾ã™---"
        )
        return "transform_query"
    else:
        # é–¢é€£ã™ã‚‹æ–‡æ›¸ãŒã‚ã‚‹ãŸã‚ã€å›ç­”ã‚’ç”Ÿæˆã—ã¾ã™
        print("---æ±ºå®š: å›ç­”ã‚’ç”Ÿæˆã—ã¾ã™---")
        return "generate"
```

#### ã‚°ãƒ©ãƒ•ã®ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã¨å®Ÿè¡Œ

æœ€å¾Œã«ã€å®šç¾©ã—ãŸãƒãƒ¼ãƒ‰ã¨ã‚¨ãƒƒã‚¸ã‚’ä½¿ç”¨ã—ã¦ã‚°ãƒ©ãƒ•ã‚’æ§‹ç¯‰ã—ã€ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã—ã¾ã™ã€‚

```python
from langgraph.graph import END, StateGraph, START

workflow = StateGraph(GraphState)

# ãƒãƒ¼ãƒ‰ã‚’å®šç¾©
workflow.add_node("retrieve", retrieve)  # æ¤œç´¢
workflow.add_node("grade_documents", grade_documents)  # æ–‡æ›¸è©•ä¾¡
workflow.add_node("generate", generate)  # ç”Ÿæˆ
workflow.add_node("transform_query", transform_query)  # ã‚¯ã‚¨ãƒªå¤‰æ›
workflow.add_node("web_search_node", web_search)  # Webæ¤œç´¢

# ã‚°ãƒ©ãƒ•ã‚’æ§‹ç¯‰
workflow.add_edge(START, "retrieve")
workflow.add_edge("retrieve", "grade_documents")
workflow.add_conditional_edges(
    "grade_documents",
    decide_to_generate,
    {
        "transform_query": "transform_query",
        "generate": "generate",
    },
)
workflow.add_edge("transform_query", "web_search_node")
workflow.add_edge("web_search_node", "generate")
workflow.add_edge("generate", END)

# ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«
app = workflow.compile()
```

![](/images/c-rag/image-2.png =50%x)

### å®Ÿè¡Œä¾‹ã¨è©•ä¾¡

#### åŸºæœ¬çš„ãªè³ªå•ã¸ã®å¯¾å¿œ

è¨˜äº‹ã®å†…å®¹ã‚’ã‚¹ãƒˆãƒ¬ãƒ¼ãƒˆã«èã„ã¦ã¿ã¾ã™ã€‚

```python
from pprint import pprint

# å®Ÿè¡Œ
inputs = {"question": "AgenticRAGã¨ã¯"}
for output in app.stream(inputs):
    for key, value in output.items():
        # ãƒãƒ¼ãƒ‰
        pprint(f"ãƒãƒ¼ãƒ‰ '{key}':")
    pprint("\n---\n")

# æœ€çµ‚çš„ãªç”Ÿæˆçµæœ
pprint(value["generation"])
```

å®Ÿè¡Œçµæœï¼š

```
---æ¤œç´¢ä¸­---
"ãƒãƒ¼ãƒ‰ 'retrieve':"
'\n---\n'
---æ–‡æ›¸ã®é–¢é€£æ€§ã‚’è©•ä¾¡ä¸­---
---è©•ä¾¡: æ–‡æ›¸ã¯é–¢é€£æ€§ã‚ã‚Š---
---è©•ä¾¡: æ–‡æ›¸ã¯é–¢é€£æ€§ã‚ã‚Š---
---è©•ä¾¡: æ–‡æ›¸ã¯é–¢é€£æ€§ã‚ã‚Š---
---è©•ä¾¡: æ–‡æ›¸ã¯é–¢é€£æ€§ã‚ã‚Š---
é–¢é€£æ€§ã®ã‚ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°: 4/4
---ASSESS GRADED DOCUMENTS---
---æ±ºå®š: å›ç­”ã‚’ç”Ÿæˆã—ã¾ã™---
"ãƒãƒ¼ãƒ‰ 'grade_documents':"
'\n---\n'
---GENERATE---
"ãƒãƒ¼ãƒ‰ 'generate':"
'\n---\n'
('AgenticRAGã¯ã€AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ™ãƒ¼ã‚¹ã®RAGå®Ÿè£…ã‚’æŒ‡ã—ã¾ã™ã€‚ç¾åœ¨ã®RAGã«é–¢ã™ã‚‹æ¦‚å¿µã¨å®Ÿè£…æ–¹æ³•ã«ã¤ã„ã¦èª¬æ˜ã•ã‚Œã¦ã„ã¾ã™ã€‚Agentic '
 'RAGã¯è¤‡æ•°ã®æƒ…å ±æºã‹ã‚‰ã®æ¤œç´¢ã‚„æ¤œç´¢çµæœã®æ¤œè¨¼ã‚’è¡Œã†ã“ã¨ãŒã§ãã¾ã™ã€‚')
```

4ã¤ã®æƒ…å ±æºã‹ã‚‰æƒ…å ±ã‚’å–å¾—ã—ã¦ã€é–¢é€£æ€§ãŒã‚ã‚‹ã®ã§ã€Webæ¤œç´¢ã‚’ã›ãšã«å›ç­”ã‚’ç”Ÿæˆã—ã¦ã„ã¾ã™ã€‚

#### é–¢é€£ã®ãªã„è³ªå•ã¸ã®å¯¾å¿œ

å…¨ãé–¢é€£ã®ãªã„è³ªå•ã‚’ã—ã¦ã¿ã¾ã™ã€‚

```python
# å®Ÿè¡Œ
inputs = {"question": "æ˜æ—¥ã®å¤©æ°—ã«ã¤ã„ã¦æ•™ãˆã¦"}
for output in app.stream(inputs):
    for key, value in output.items():
        # ãƒãƒ¼ãƒ‰
        pprint(f"ãƒãƒ¼ãƒ‰ '{key}':")
    pprint("\n---\n")

# æœ€çµ‚çš„ãªç”Ÿæˆçµæœ
pprint(value["generation"])
```

å®Ÿè¡Œçµæœï¼š

```
---æ¤œç´¢ä¸­---
"ãƒãƒ¼ãƒ‰ 'retrieve':"
'\n---\n'
---æ–‡æ›¸ã®é–¢é€£æ€§ã‚’è©•ä¾¡ä¸­---
---è©•ä¾¡: æ–‡æ›¸ã¯é–¢é€£æ€§ãªã—---
---è©•ä¾¡: æ–‡æ›¸ã¯é–¢é€£æ€§ãªã—---
---è©•ä¾¡: æ–‡æ›¸ã¯é–¢é€£æ€§ãªã—---
---è©•ä¾¡: æ–‡æ›¸ã¯é–¢é€£æ€§ãªã—---
é–¢é€£æ€§ã®ã‚ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°: 0/4
---ASSESS GRADED DOCUMENTS---
---æ±ºå®š: ã™ã¹ã¦ã®æ–‡æ›¸ãŒè³ªå•ã«é–¢é€£ã—ã¦ã„ãªã„ãŸã‚ã€ã‚¯ã‚¨ãƒªã‚’å¤‰æ›ã—ã¾ã™---
"ãƒãƒ¼ãƒ‰ 'grade_documents':"
'\n---\n'
---TRANSFORM QUERY---
å…ƒã®è³ªå•: æ˜æ—¥ã®å¤©æ°—ã«ã¤ã„ã¦æ•™ãˆã¦
æ›¸ãæ›ãˆã‚‰ã‚ŒãŸè³ªå•: æ˜æ—¥ã®å¤©æ°—äºˆå ±ã‚’æ•™ãˆã¦ãã ã•ã„ã€‚
"ãƒãƒ¼ãƒ‰ 'transform_query':"
'\n---\n'
---Webæ¤œç´¢ä¸­---
ã‚¦ã‚§ãƒ–æ¤œç´¢çµæœ: 08æ—¥(åœŸ)  æ˜æ—¥ 08æ—¥(åœŸ)  æ˜æ—¥ ä¿¡é ¼åº¦ -   -   -   C   B   A   B   C ä¿¡é ¼åº¦ -   -   -   C   B   A   B   C 08æ—¥(åœŸ)  æ˜æ—¥ ä¿¡é ¼åº¦ -   -   -   C   B   A   C   C 08æ—¥(åœŸ)  æ˜æ—¥ ä¿¡é ¼åº¦ -   -   -   A   B   C   A   A 08æ—¥(åœŸ)  æ˜æ—¥ ä¿¡é ¼åº¦ -   -...
"ãƒãƒ¼ãƒ‰ 'web_search_node':"
'\n---\n'
---GENERATE---
"ãƒãƒ¼ãƒ‰ 'generate':"
'\n---\n'
'æ˜æ—¥ã®å¤©æ°—ã¯ã€æ±åŒ—ã§ã¯é›ªã‚„é›¨ãŒé™ã‚Šã€å¤ªå¹³æ´‹å´ã§ã¯å¤§é›ªã®ãŠãã‚ŒãŒã‚ã‚Šã¾ã™ã€‚é–¢æ±ã¯é›¨ãŒé™ã‚‹è¦‹è¾¼ã¿ã§ã€æ±æµ·ã‚„è¥¿æ—¥æœ¬ã¯æ›‡ã‚ŠãŒç¶šãã§ã—ã‚‡ã†ã€‚å…¨å›½ã®å¤©æ°—æƒ…å ±ã«ã‚ˆã‚‹ã¨ã€æ˜æ—¥ã®å¤©æ°—ã¯å„åœ°ã§ç•°ãªã‚‹çŠ¶æ³ãŒäºˆæƒ³ã•ã‚Œã¾ã™ã€‚'
```

ãã¡ã‚“ã¨Webæ¤œç´¢ã—ã¦æ­£ã—ã„å›ç­”ã‚’è¿”ã—ã¦ã„ã¾ã™ã­ã€‚

## ã¾ã¨ã‚
ã“ã“ã¾ã§èª­ã‚“ã§ã„ãŸã ãã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã—ãŸï¼

ä»Šå›ã®å®Ÿè£…ã§ã¯ã€æŠ½å‡ºã—ãŸæƒ…å ±ãŒã™ã¹ã¦é–¢é€£æ€§ãŒã‚ã‚‹ã¨åˆ¤æ–­ã•ã‚ŒãŸæ™‚ã®ã¿ã€æŠ½å‡ºã—ãŸæ–‡ç« ã®ã¿ä½¿ã„ã€ä¸€ã¤ã§ã‚‚é–¢é€£æ€§ãŒãªã„ã¨åˆ¤æ–­ã•ã‚ŒãŸã‚‰Webæ¤œç´¢ã‚’è¡Œã„ã€ãã®çµæœã‚’ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«è¿½åŠ ã—ã¦ã„ã¾ã—ãŸã€‚ã“ã®è¾ºã‚Šã®ãƒ­ã‚¸ãƒƒã‚¯ã¯å®Ÿéš›ã«RAGã‚’è¡Œã†ã¨ãã®è¦ä»¶ã«ã‚ˆã£ã¦ã‚‚å¤‰ã‚ã£ã¦ããã†ã§ã™ã€‚

:::message
ã€ãŠä»•äº‹ã®ã”ä¾é ¼ã€‘
å¼Šç¤¾ã§ã¯ç”Ÿæˆ AI ã‚’ç”¨ã„ãŸã‚·ã‚¹ãƒ†ãƒ é–‹ç™ºã‚’æ”¯æ´ã—ã¦ã„ã¾ã™ã€‚
ã‚‚ã—ãŠä»•äº‹ã®ç›¸è«‡ãŒã‚ã‚Œã°ã“ã¡ã‚‰ãƒ•ã‚©ãƒ¼ãƒ ã‚ˆã‚ŠãŠæ°—è»½ã«ã”ç›¸è«‡ãã ã•ã„ï¼
https://forms.gle/RW1Kpaxwx64RBmgL6

ãƒ¡ãƒ¼ãƒ«ã§ã®å•ã„åˆã‚ã›ã¯ã“ã¡ã‚‰ã‹ã‚‰ã‚ˆã‚ã—ããŠé¡˜ã„ã„ãŸã—ã¾ã™ã€‚
sakai.shun@egghead.co.jp
:::
